{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2\n",
    "\n",
    "## Project 5\n",
    "\n",
    "#### Quang, Paul and Luke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our token acquired from the youtube api, we set about acquiring data that interested us from youtube. To that end, we decided to primarily focus on time of upload, and categories of video. For each video uploaded, youtube and the content creator assigns a category id to the video, denoting the type of video you will be watching. These broad categories range from politics, to entertainment, animals and pets, and everything in between. We decided to get our data from the mostPopular videos of each day, so we could see if there was a prime upload time, either in general or specific to each category, that would lend a video a more likely chance of success. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data acquisition process comes from one primary main() function, with 12 subsidary helper functions. This notebook will take you through these functions, and in turn, how we went about getting and organizing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from requests_oauthlib import OAuth2Session\n",
    "import keys\n",
    "from sqlalchemy import *\n",
    "import keysSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our main function. Initially, when we were going about our exploration of the youtube api, we were only looking at the most popular videos globally. However, as the project progressed, we decided that it would be interesting to not only find data for the most popular videos specific to a region, but look at and compare the most popular videos between a few regions at once, without having to create multiple relational tables. Therefore, we designed our main() function so it could either take no inputs and default to gather global data, or take any number of specified regions, and gather all of their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(countries = []):\n",
    "    \"\"\"\n",
    "    This function gathers data from the youtube api about their current\n",
    "    most popular videos, and can be narrowed to look at either a specific\n",
    "    region, or a group of them. It then sends the data to an sql database\n",
    "    \n",
    "    Parameters:\n",
    "    Optionally a list of countries to gather local data\n",
    "    \n",
    "    Return Values:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if countries == []:\n",
    "        df=query()\n",
    "    #gathers global data if no country is specified\n",
    "        \n",
    "        engine,connection,metadata=sqlLoadIn()\n",
    "        toSql(engine,connection,metadata,df)\n",
    "    \n",
    "    else:\n",
    "        country_concat = []\n",
    "        string=''\n",
    "        for country in countries:\n",
    "            string+=country+'_'\n",
    "            df = query(country)\n",
    "            country_concat.append(df)\n",
    "    #gathers each countries data from the parameter lists, and compiles\n",
    "    #them in one master dictionary\n",
    "    \n",
    "        df=pd.concat(country_concat, ignore_index=True)\n",
    "    \n",
    "        engine,connection,metadata=sqlLoadIn()\n",
    "        toSql(engine,connection,metadata,df,string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our main() function, the first helper function hierarchically is our query() function. This function serves a few purposes. First, it collects our authorization token from our loadIn() function, and then tests if our token is still valid for collecting data. If not, it automatically gathers a refreshed token, and goes through the acquisition process again. Finally, it merges the data pulled from the youtube api with another table we've generated to match the category idea with the name of what each id means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query(country=\"\"):\n",
    "    \"\"\"\n",
    "    This function queries the youtube api for data about their current\n",
    "    most popular videos, and returns that information in a dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    Optionally a country to gather local data\n",
    "    \n",
    "    Return Values:\n",
    "    A dataframe\n",
    "    \"\"\"\n",
    "    token=loadIn()\n",
    "    try:\n",
    "        df=getData(token,country)\n",
    "    #this try except loops tests if the authorization token is valid,\n",
    "    #and refreshes it if not\n",
    "        \n",
    "        result=merge(token,df,country)\n",
    "        return result\n",
    "    \n",
    "    except KeyError:\n",
    "        new_token=refreshToken(token)\n",
    "    \n",
    "        df=getData(new_token,country)\n",
    "        result = merge(new_token,df,country)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refreshToken() does exactly what the name says, and refreshes our token if it is currently expired. To do this we need access to our keys.py, as well as our current OAuth session and refresh_url, all of which we get from the helper function keychain()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def refreshToken(token):\n",
    "    \"\"\"\n",
    "    This function takes an expired token, and refreshes the expired token\n",
    "    with the refresh token\n",
    "    \n",
    "    Paremeters:\n",
    "    An expired token\n",
    "    \n",
    "    Return Value:\n",
    "    A new token\n",
    "    \"\"\"\n",
    "    keychain_1,session,refresh_url=keychain()\n",
    "    #Uses the helper function keychain to pull needed information out to refresh\n",
    "    #the token\n",
    "    \n",
    "    token = session.refresh_token(refresh_url, \n",
    "                                 client_id=keychain_1['youtube']['client_id'],\n",
    "                                 client_secret=keychain_1['youtube']['client_secret'],\n",
    "                                 refresh_token=token['refresh_token'])\n",
    "    #creates a refreshed session with the youtube api using the refresh token\n",
    "    #to allowed continued access and authorization\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keychain() shares the burden with loadIn() for setting up most of the structural framework we'll need for this project. The main point of this function is setting up an OAuth2 session, but it also takes care of a few specifics which can then be passed to refreshToken() to let it run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keychain():\n",
    "    \"\"\"\n",
    "    This function opens a keys.py file in the same directory, and using that keychain, creates an OAuth2 session with our api\n",
    "    \n",
    "    Parameters:\n",
    "    None\n",
    "    \n",
    "    Return Values:\n",
    "    The keys.py file as keychain, an OAuth2 session, and a refresh_url\n",
    "    \"\"\"\n",
    "    importlib.reload(keys)\n",
    "    keychain = keys.keychain\n",
    "    client_id = keychain['youtube']['client_id']\n",
    "    scope = keychain['youtube']['scope']\n",
    "    redirect = keychain['youtube']['redirect_uris'][0]\n",
    "    refresh_url = keychain['youtube']['token_uri']\n",
    "    session = OAuth2Session(client_id, scope=scope, redirect_uri=redirect)\n",
    "    return keychain,session,refresh_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the function we used to load in our token from our previous notebook. Using this method, we can access it at any point throughout this notebook, or any others we would need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadIn():\n",
    "    \"\"\"\n",
    "    This function opens a saved token.json file in the same directory, and reads it in in json\n",
    "    \n",
    "    Parameters:\n",
    "    None\n",
    "    \n",
    "    Return Values:\n",
    "    A json object\n",
    "    \"\"\"\n",
    "    token=open('token.json',mode='r')\n",
    "    token=token.read()\n",
    "    token=json.loads(token)\n",
    "    #turns the token from a string object into a json one\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name denotes, getData() is our primary data gathering function. Once this function takes a valid token, passed to it from query(), it uses a helper function to generate the proper url to use a get request to pull the data from the youtube api, and then uses another helper function to arrange it into a tidy dataframe. This tidy dataframe with the youtube data is then passed back to query()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData(token,country=\"\"):\n",
    "    \"\"\"\n",
    "    This function, with the use of several helper functions, \n",
    "    queries the youtube api, retrieves data, and sorts it into a dictionary.\n",
    "    This function is the parent function of createUrl and arrange.\n",
    "    \n",
    "    Parameters:\n",
    "    A valid authorization token, and optionally a country to gather local data \n",
    "    \n",
    "    Return Values:\n",
    "    A dataframe\n",
    "    \"\"\"\n",
    "    data = {'videoId':[], 'channelId':[],'categoryId':[] ,'channelTitle':[], 'publishedAt':[]}\n",
    "    page = []\n",
    "    popular_vid,D,url=createUrl(token,country)\n",
    "    #Uses the helper function createUrl to get information from the youtube api\n",
    "    \n",
    "    for i in range(10):\n",
    "        if 'nextPageToken' not in popular_vid:\n",
    "            next_page = 'CAUQAA'\n",
    "        else:\n",
    "            next_page = popular_vid['nextPageToken']\n",
    "    #creates a for loop to loop through consecutive pages of information from the youtube api\n",
    "    #using the nextPageToken\n",
    "        \n",
    "        page.append(next_page)\n",
    "        D['pageToken'] = next_page\n",
    "        popular_vid = requests.get(url, params = D)\n",
    "    #requests new pages of information from the youtube api by appending the url\n",
    "        \n",
    "        popular_vid = popular_vid.json()\n",
    "        arrange(data,popular_vid['items'],country)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our createUrl() function, as the names implies, creates the url we will be using to make a get request to the youtube api. Additionally however, it also passes a dictionary back to it's parent function with the parameters of the url so the information gathered can be more easily sorted, as well as a json object with the initial page of information from the api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createUrl(token,country=\"\"):\n",
    "    \"\"\"\n",
    "    This function creates a url, which it uses as part of a get \n",
    "    request to query the youtube api for information about the most\n",
    "    popular videos in a given region.\n",
    "    \n",
    "    Parameters:\n",
    "    A valid authorization token, and optionally a country to gather local data \n",
    "    \n",
    "    Return Values:\n",
    "    A json object with the queried data, a dictionary with the search paramters, and the created url\n",
    "    \"\"\"\n",
    "    D={}\n",
    "    D['access_token'] = token['access_token']\n",
    "    D['part'] = ['snippet,contentDetails,statistics']\n",
    "    D['maxResults'] = '50'\n",
    "    D['chart'] ='mostPopular'\n",
    "    D['PageToken'] = 'CAUQAA'\n",
    "    if country != \"\":\n",
    "        D[\"regionCode\"] = country\n",
    "    #checks if a country was specified\n",
    "    \n",
    "    url ='https://www.googleapis.com/youtube/v3/videos'\n",
    "    popular_vid = requests.get(url, params = D)\n",
    "    #requests the information for the youtube api\n",
    "    \n",
    "    popular_vid = popular_vid.json()\n",
    "    #turns the recieved information into a json object\n",
    "   \n",
    "    return popular_vid,D,url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastely from getData() is our arrange() function. The functionality of arrange() is farily simple. It takes all of the data we have gathered thus far from the api as raw json, and sorts it into a dictionary before returning it back to getData(), and subsequently, query(). Having it in this form allows us to easily put it in a dataframe, and then upload it to a sql datbase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arrange(data,vid,country=\"\"):\n",
    "    \"\"\"\n",
    "    This function sorts the gathered data into a dictionary, so it can later be passed into a data frame\n",
    "    \n",
    "    Parameters:\n",
    "    A dictionray, the gathered data, and optionally the country the data was gathered from\n",
    "    \n",
    "    Return Values:\n",
    "    A dictionary\n",
    "    \"\"\"\n",
    "    if country != \"\" and \"CountryCode\" not in data:\n",
    "        data[\"CountryCode\"] = []\n",
    "    #checks if a country was specified\n",
    "    \n",
    "    for item in vid:\n",
    "        data['videoId'].append(item['id'])\n",
    "        data['channelId'].append(item['snippet']['channelId'])\n",
    "        data['categoryId'].append(item['snippet']['categoryId'])\n",
    "        data['channelTitle'].append(item['snippet']['channelTitle'])\n",
    "        data['publishedAt'].append(item['snippet']['publishedAt'][11:13])\n",
    "        if country != \"\":\n",
    "            data['CountryCode'].append(country)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge() is a parent function to two seperate functions, categories() and catgoryTable(), who's purpose is to take our earlier ditctionary with all our data on the popular regional videos, and merge it with a new dictionary pairing category ids to their string, resulting in one unified dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge(token,df,country=\"\"):\n",
    "    \"\"\"\n",
    "    This function, with the use of several helper functions,\n",
    "    queries the youtube api so it can merge two dataframes\n",
    "    together, and apply correct category labels.\n",
    "    \n",
    "    Paramters:\n",
    "    A valid authorization token, a dataframe, and optionally \n",
    "    a country\n",
    "    \n",
    "    Return Values:\n",
    "    A dataframe\n",
    "    \"\"\"\n",
    "    category_vid=categories(token)\n",
    "    df1=categoryTable(category_vid['items'])\n",
    "    \n",
    "    result = pd.merge(df, df1, left_on='categoryId', right_on='id')\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categories() serves a similar function as getData(), but less complicated. This function sends a get request to the youtube api, but instead of gathering live data on videos, it pulls information about the category ids on youtube, passes it into a json ojbect, and then returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categories(token,country=\"\"):\n",
    "    \"\"\"\n",
    "    This function queries the youtube api for the names of each categoryID\n",
    "    \n",
    "    Paramters:\n",
    "    A valid authorization token, and optionally a country to gather local data \n",
    "    \n",
    "    Return Value:\n",
    "    A json object\n",
    "    \"\"\"\n",
    "    url = 'https://www.googleapis.com/youtube/v3/videoCategories'\n",
    "    C = {}\n",
    "    C['access_token'] = token['access_token']\n",
    "    C['part'] = 'snippet'\n",
    "    if country!=\"\":\n",
    "        C['regionCode']=country\n",
    "    #checks if a country was specified\n",
    "    \n",
    "    C['regionCode'] = 'US'\n",
    "    category_vid = requests.get(url, params = C)\n",
    "    category_vid = category_vid.json()\n",
    "    return category_vid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categoryTable() takes a json object, in this case the one created by categories(), and converts it from json to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categoryTable(json):\n",
    "    \"\"\"\n",
    "    This function takes a json object and sorts it into a dictionary\n",
    "    \n",
    "    Parameters:\n",
    "    A json object\n",
    "    \n",
    "    Return Values:\n",
    "    A dataframe\n",
    "    \"\"\"\n",
    "    data_category = {'id':[],'title':[]}\n",
    "    for item in json:\n",
    "        data_category['id'].append(item['id'])\n",
    "        data_category['title'].append(item['snippet']['title'])\n",
    "    return pd.DataFrame(data_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have gotten the data we wanted from the youtube api, sorted it into a dictionary, and merged it together into one unified data frame, so each row gives us the infomration about the video, the time uploaded, and what category of video the upload was. Initally when we reached this point, we exported the data as a .csv file, and uploaded that information locally to tableau to begin visualizing what we'd learned and gathered from our data acquisition. However, we decided that we might be better served by instead creating a a connection to an sql server, and uploading a database of our data to the server, and accessing it through there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqlLoadIn() creates the necessary engine, connection, and metadata to connect to a sql server, as well as upload information. It then returns these pieces so a subsequent function can access them for pushing things to the this server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sqlLoadIn():\n",
    "    \"\"\"\n",
    "    This function loads in the necessary data to connect to an\n",
    "    sql server\n",
    "    \n",
    "    Parameters:\n",
    "    None\n",
    "    \n",
    "    Return Values:\n",
    "    An engine object, a connection, metadata\n",
    "    \"\"\"\n",
    "    keychain = keysSQL.keychain\n",
    "    protocol = \"mysql+mysqlconnector\"\n",
    "    userid = keychain[\"MySQL\"][\"userid\"]\n",
    "    userpass = keychain[\"MySQL\"][\"userpass\"]\n",
    "    mysqlhost = \"hadoop2.mathsci.denison.edu\"\n",
    "    database = \"rubens_p1\"\n",
    "    connectionstring = \"{}://{}:{}@{}/{}\".format(protocol, userid, userpass, mysqlhost, database)\n",
    "    engine = create_engine(connectionstring)\n",
    "    connection = engine.connect()\n",
    "    metadata = MetaData()\n",
    "    return engine,connection,metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, toSql() sends the dataframe we created earlier to the sql server we have established the connection to. The main issue that needed solving here, and what made the function so long, was accounting for whether countries had been specified in the initial paramter of main(). If it had, we first needed to change the table we were creating to include, or exclude a CountryCode column, to keep the lenght of the table the same as the dataframe. Then, we had to make sure that each table had a unique name when it was uploaded to the server, as to not overwite previously uploaded, but different talbes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSql(engine,connection,metadata,df,loc='global'):\n",
    "    \"\"\"\n",
    "    This function takes a connection to an sql server and a\n",
    "    dataframe, and turns the dataframe object into a database\n",
    "    on the server.\n",
    "    \n",
    "    Parameters:\n",
    "    An engine to database, a connection to a database,\n",
    "    metadata for managing the connection, a dataframe, and \n",
    "    optionally a country name if specified.\n",
    "    \n",
    "    Return Values:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if loc!='global':\n",
    "    #checks if countires have been specified, and gives the database\n",
    "    #a unique name if so\n",
    "    \n",
    "        dropTable = text(\"DROP TABLE IF EXISTS \"+loc)\n",
    "        connection.execute(dropTable)\n",
    "    #This sees if an identically named table exists, and deletes it if\n",
    "    #that's the case to avoid errors\n",
    "\n",
    "        country_cate = Table(loc, metadata,\n",
    "                         Column('CountryCode', String(255)),\n",
    "                         Column('categoryId', Integer()),\n",
    "                         Column('channelId', String(255)),\n",
    "                         Column('channelTitle', String(255)),\n",
    "                         Column('publishedAt', Integer()),\n",
    "                         Column('videoId', String(255)),\n",
    "                         Column('Id', Integer()),\n",
    "                         Column('title', String(255)),extend_existing=True)\n",
    "\n",
    "        metadata.create_all(engine)\n",
    "        columnList = df.values.tolist()\n",
    "\n",
    "        for row in range(len(df)):\n",
    "            stmt = insert(country_cate).values(CountryCode = columnList[row][0], \n",
    "                                       categoryId = columnList[row][1], \n",
    "                                       channelId = columnList[row][2],\n",
    "                                       channelTitle = columnList[row][3], \n",
    "                                       publishedAt = columnList[row][4],\n",
    "                                       videoId = columnList[row][5],\n",
    "                                       Id = columnList[row][6],\n",
    "                                       title = columnList[row][7])\n",
    "\n",
    "            connection.execute(stmt)\n",
    "    \n",
    "    else:\n",
    "    #if no country is specified, the CountryCode column needs to be removed to\n",
    "    #keep the index in range\n",
    "    \n",
    "        dropTable = text(\"DROP TABLE IF EXISTS global\")\n",
    "        connection.execute(dropTable)\n",
    "        \n",
    "        country_cate = Table(\"global\", metadata,\n",
    "                        Column(\"categoryId\", Integer()),\n",
    "                        Column(\"channelId\", String(255)),\n",
    "                        Column(\"channelTitle\", String(255)),\n",
    "                        Column(\"publishedAt\", Integer()),\n",
    "                        Column(\"videoId\", String(255)),\n",
    "                        Column(\"Id\", Integer()),\n",
    "                        Column(\"title\", String(255)),extend_existing=True)\n",
    "        \n",
    "        metadata.create_all(engine)\n",
    "        columnList = df.values.tolist()\n",
    "\n",
    "        for row in range(len(df)):\n",
    "            stmt = insert(country_cate).values(categoryId = columnList[row][0], \n",
    "                                       channelId = columnList[row][1],\n",
    "                                       channelTitle = columnList[row][2], \n",
    "                                       publishedAt = columnList[row][3],\n",
    "                                       videoId = columnList[row][4],\n",
    "                                       Id = columnList[row][5],\n",
    "                                       title = columnList[row][6])\n",
    "            \n",
    "            connection.execute(stmt)\n",
    "   \n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(['ES','FR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, this leaves us with one main() function, and a database table on an sql server. As our scope of what we wanted to include in the project evolved, so to did the complexity. Changing the notebook from something unorganized and full of gloabl varialbes into one with one main function with subsidary helper functions provided some unforseen challenges, as we had to make sure that each helper function had the appropriate token, and could handle if whether or not we had specified countries. One of the last changes we did was implementing the ability to pull information on multiple countries into one dataframe, which required us to turn query, our old main() function, into a child of our new main() function. Although it made the final product quite lenghty, having so many abstractions and helper functions ended up being one of our strengths, as it let us more surgically make changes to specific parts of the process, without having to fundementally change the whole flow of the notebook. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
